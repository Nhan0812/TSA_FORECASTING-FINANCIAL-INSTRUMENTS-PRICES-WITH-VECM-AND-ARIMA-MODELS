---
title: "TSA Report"
date: "2023-06-01"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache   = TRUE,
                      message = FALSE, 
                      warning = FALSE)
```

```{r}
getwd()
```

```{r}
setwd("D://UW//2. Summer 22-23//1. Time Series Analysis//Project//TSA")
```

# 1. Data Preparation

Load necessary packages to memory:

```{r}
library(xts)
library(tidyverse)
library(quantmod)
library(lmtest)
library(forecast)
library(dygraphs)
library(vars)
library(knitr)
library(kableExtra)
```

Load some pre-defined functions:

```{r}
source("testdf.R")
```

```{r}
source("function_plot_ACF_PACF_resids.R")
```

Load Dataset:

```{r}
Data <- read.csv("TSA_2023_project_data_1.csv",header = TRUE, dec = ".")
```

# 2. Summary Data

```{r}
str(Data)
```

Notice that the first column is the Date column, which is under
"Character" type We need to transform it into "Date" type.

```{r}
Data$X <- as.Date(Data$X, 
                  format = "%d-%m-%y")
```

Until now the class is "Data.Frame" object

```{r}
class(Data) 
```

Create xts objects

```{r}
Data <- xts(Data[,-1], Data$X)
```

After creating xts objects, the class is "xts", "zoo"

```{r}
class(Data) 
```

```{r}
str(Data)
```

```{r}
Data %>% glimpse()
```

Checking for missing data

```{r}
summary(Data)
```

```{r}
head(Data,6)
```

There is no missing data for the table.

# 3. Checking for Cointegration

## Visualization

Plot 10 Time Series together

```{r}
plot(Data,  
     main = "Graph for 10 Time Series",  
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     legend.loc = "bottomleft",
     type="l")
```

Plot 10 Time Series separately

```{r}
plot(Data,
     main = "Graph for 10 Time Series",  
     major.ticks = "years", 
     grid.ticks.on = "years",
     legend.loc = "bottomleft",
     multi.panel = TRUE,
     yaxis.same = FALSE,
     type="l")
```

So our group decide to choose Time Series 1 and 2 for analysis.

## Create first difference

```{r}
Data$dx1 <- diff.xts(Data$x1)
Data$dx2 <- diff.xts(Data$x2)
```

Plot both variables on the graph:

```{r}
plot(Data[, 1:2],
     col = c("black", "blue"),
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 2,
     main = "Time Series X1 & X2",
     legend.loc = "topright")
```

## Test cointegration

We perform the tests of integration order.

Testing the order of the time series 1 and its difference:

```{r}
testdf(variable = Data$x1,
       max.augmentations = 3)
```

Interpret result: The p-bg is very small -\> there are auto-correlation
in residuals, even when we add the augmentations, we still cant get rid
of auto-correlation. Hence, we test for its 1st difference.

```{r}
testdf(variable = Data$dx1, 
       max.augmentations = 3)
```

The p-bg is greater than 5% -\> there are no auto-correlations in
residual Next, we test the stationary of its 1st lag, by checking p-adf.
p-adf is smaller than 5%. We can reject the null in the case of the
first differences about non-stationary. Its 1st lag is stationary. We
can conclude that the Time Series X1 is integrated of order 1.

Testing the order of the time series 2 and its difference:

```{r}
testdf(variable = Data$x2,
       max.augmentations = 3)
```

Interpret result: Similar to X1, the p-bg here is smaller than 5% -\>
there are auto-correlation in residuals, even when we add the
augmentations, we still cant get rid of auto-correlation. Hence, we test
for its 1st difference.

```{r}
testdf(variable = Data$dx2, 
       max.augmentations = 3)
```

By adding 1 augmentation, there will be no auto correlation in
Residuals. p-adf is greater than 5%, we can reject the null hypothesis
in the case the non-stationary of first differences. Its 1st lag is
stationary. we can conclude that the Time Series X2 is integrated of
order 1.

**Conclusion:** As a result, both variables has the same order 1: âˆ¼I(1)
so in the next step we can check whether they are cointegrated or not.

## Estimate Cointegrated Vector

```{r}
model.coint <- lm(x1 ~ x2, data = Data)
```

Examine the model summary:

```{r}
summary(model.coint)
```

Both the intercept and Coefficient for x2 are statistically significant.

The model is significantly explained by x2. We further test the
stationary of the residuals:

```{r}
testdf(variable = residuals(model.coint), max.augmentations = 3)
```

The ADF test with no augmentations can be used. The result is that
non-stationarity of residuals is STRONGLY REJECTED, so residuals are
stationary, which means that x1 and x2 are cointegrated.

The cointegrating vector is [1, -29.853 , -0.713]

which defines the cointegrating relationship as: 1 \* x1 - 29.853 -
0.713 \* x2.

# 4. Applying Box-Jenkins Procedure

## 4.1 Applying for Time Series X1

### Step 1: Model Parameters

```{r}
par(mfrow = c(2, 1)) 
acf(Data$dx1,
    lag.max = 36, # max lag for ACF
    ylim = c(-0.1, 0.1),   # limits for the y axis - we give c(min, max)
    lwd = 5,               
    col = "dark green",
    na.action = na.pass)   # do not stop if there are missing values in the data
pacf(Data$dx1, 
     lag.max = 36, 
     lwd = 5, col = "dark green",
     na.action = na.pass)

par(mfrow = c(1, 1)) # restore the original single panel
```

The PACF shown is suggestive of an AR(5) model. So an initial candidate
model is an ARIMA(5,1,0). We also have some variations of this model:
ARIMA(5,1,1), ARIMA(4,1,0),ARIMA(3,1,0), ARIMA(3,1,3).

### Step 2: Model Estimation

```{r}
arima510 <- Arima(Data$x1,  # variable	
                  order = c(5, 1, 0)  # (p,d,q) parameters	
)
```

```{r}
arima313 <- Arima(Data$x1,  # variable	
                  order = c(3, 1, 3)  # (p,d,q) parameters	
)
```

```{r}
coeftest(arima510)
summary(arima510)
```

The above model is not included constant. Let's include constant into
the model:

```{r}
arima510_2 <- Arima(Data$x1,  # variable	
                  order = c(5, 1, 0),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima510_2)
```

Adding the constant did not change the result much, so we keep the model
without constant

### Step 3: Model Diagnostics

```{r}
plot(resid(arima510))
```

```{r}
plot(resid(arima313), col = "royalblue3")
```

Lets check the ACF and the PACF of the Residual values:

```{r}
plot_ACF_PACF_resids(arima510)
```

The Ljung-Box test:

```{r}
Box.test(resid(arima510), type = "Ljung-Box", lag = 10)
Box.test(resid(arima510), type = "Ljung-Box", lag = 15)
Box.test(resid(arima510), type = "Ljung-Box", lag = 20)
Box.test(resid(arima510), type = "Ljung-Box", lag = 25)
```

```{r}
Box.test(resid(arima313), type = "Ljung-Box", lag = 10)
Box.test(resid(arima313), type = "Ljung-Box", lag = 15)
Box.test(resid(arima313), type = "Ljung-Box", lag = 20)
Box.test(resid(arima313), type = "Ljung-Box", lag = 25)
```

-\> Very large p-values: The residual is white-noise

Plot graph for Ljung-Box test:

```{r}
bj_pvalues = c()
```

```{r}
for(i in c(1:100)){
  bj = Box.test(resid(arima313), type = "Ljung-Box", lag = i)
  bj_pvalues = append(bj_pvalues,bj$p.value)
}

plot(bj_pvalues, type='l')

abline(h = 0.05, col='red')

```

Model ARIMA(5,1,1)

```{r}

arima511 <- Arima(Data$x1,  # variable	
                    order = c(5, 1, 1),  # (p,d,q) parameters
                    include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima511)
```

ARIMA(4,1,0)

```{r}
arima410 <- Arima(Data$x1,  # variable	
                  order = c(4, 1, 0),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima410)
```

ARIMA(3,1,0).

```{r}
arima310 <- Arima(Data$x1,  # variable	
                  order = c(3, 1, 0),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima310)
```

###Step 4. Evaluate Model

```{r}
AIC(arima510,arima510_2, arima511, arima410, arima310, arima313)
BIC(arima510, arima510_2, arima511, arima410, arima310, arima313)
```

-\> Suggestion ARIMA(3,1,3)

Cross check with Auto Correlation:

```{r}

arima.best.AIC <- 
  auto.arima(Data$x1,
             d = 1,             # parameter d of ARIMA model
             max.p = 6,         # Maximum value of p
             max.q = 6,         # Maximum value of q
             max.order = 12,    # maximum p+q
             start.p = 1,       # Starting value of p in stepwise procedure
             start.q = 1,       # Starting value of q in stepwise procedure
             ic = "aic",        # Information criterion to be used in model selection.
             stepwise = FALSE,  # if FALSE considers all models
             allowdrift = TRUE, # include a constant
             trace = TRUE)      # show summary of all models considered
```

Return: Best model: ARIMA(4,1,2)

```{r}
arima412 <- Arima(Data$x1,  # variable	
                  order = c(4, 1, 2),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima412)
```

```{r}
AIC(arima313,arima412)
BIC(arima313,arima412)
```

Check for BIC best:

```{r}
arima.best.BIC <- 
  auto.arima(Data$x1,
             d = 1,             # parameter d of ARIMA model
             max.p = 6,         # Maximum value of p
             max.q = 6,         # Maximum value of q
             max.order = 12,    # maximum p+q
             start.p = 1,       # Starting value of p in stepwise procedure
             start.q = 1,       # Starting value of q in stepwise procedure
             ic = "bic",        # Information criterion to be used in model selection.
             stepwise = FALSE,  # if FALSE considers all models
             allowdrift = TRUE, # include a constant
             trace = TRUE)      # show summary of all models considered
```

Return: Best model: ARIMA(3,1,2)

```{r}

arima312 <- Arima(Data$x1,  # variable	
                  order = c(3, 1, 2),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima312)
```

```{r}
AIC(arima313,arima412,arima312)
BIC(arima313,arima412,arima312)
```

***Conclusion:*** ARIMA(3,1,3) for forcasting Time Series X1.

## 4.1 Applying for Time Series X2

### Step 1: Model Parameters

```{r}
par(mfrow = c(2, 1)) 
acf(Data$dx2,
    lag.max = 36, # max lag for ACF
    ylim = c(-0.1, 0.1),   # limits for the y axis - we give c(min, max)
    lwd = 5,               
    col = "dark green",
    na.action = na.pass)   # do not stop if there are missing values in the data
pacf(Data$dx2, 
     lag.max = 36, 
     lwd = 5, col = "dark green",
     na.action = na.pass)

par(mfrow = c(1, 1)) # restore the original single panel

```

The PACF shown is suggestive of an AR(5) model. So an initial candidate
model is an ARIMA(5,1,0). We also have some variations of this model:
ARIMA(5,1,1), ARIMA(4,1,0),ARIMA(3,1,0).

### Step 2: Model Estimation

```{r}
arima510_x2 <- Arima(Data$x2,  # variable	
                  order = c(5, 1, 0)  # (p,d,q) parameters
)	
coeftest(arima510_x2)
summary(arima510_x2)
```

The above model is not included constant. Let's include constant into
the model:

```{r}
arima510_2_x2 <- Arima(Data$x2,  # variable	
                    order = c(5, 1, 0),  # (p,d,q) parameters
                    include.constant = TRUE)  # including a constant

coeftest(arima510_2_x2)
```

Adding the constant did not change the result much, so we keep the model
without constant

### Step 3: Model Diagnostics

```{r}
plot(resid(arima510_x2))
```

The Ljung-Box test:

```{r}
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 25)
```

-\> Very large p-values: The residual is white-noise

Plot graph for Ljung-Box test:

```{r}
bj_pvalues = c()

for(i in c(1:100)){
  bj = Box.test(resid(arima510_x2), type = "Ljung-Box", lag = i)
  bj_pvalues = append(bj_pvalues,bj$p.value)
}

plot(bj_pvalues, type='l')

abline(h=0.05, col='red')
```

Model ARIMA(5,1,1)

```{r}

arima511_x2 <- Arima(Data$x2,  # variable	
                  order = c(5, 1, 1),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima511_x2)
```

ARIMA(4,1,0)

```{r}
arima410_x2 <- Arima(Data$x2,  # variable	
                  order = c(4, 1, 0),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima410_x2)
```

ARIMA(3,1,0).

```{r}
arima310_x2 <- Arima(Data$x2,  # variable	
                  order = c(3, 1, 0),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant

coeftest(arima310_x2)
```

### Step 4. Evaluate Model

```{r}
AIC(arima510_x2,arima510_2_x2, arima511_x2, arima410_x2, arima310_x2)
BIC(arima510_x2, arima510_2_x2, arima511_x2, arima410_x2, arima310_x2)

```

-\> Suggestion model: arima510_x2 - ARIMA(5,1,0)

Cross check with Auto Correlation:

```{r}

arima.best.AIC <- 
  auto.arima(Data$x2,
             d = 1,             # parameter d of ARIMA model
             max.p = 6,         # Maximum value of p
             max.q = 6,         # Maximum value of q
             max.order = 12,    # maximum p+q
             start.p = 1,       # Starting value of p in stepwise procedure
             start.q = 1,       # Starting value of q in stepwise procedure
             ic = "aic",        # Information criterion to be used in model selection.
             stepwise = FALSE,  # if FALSE considers all models
             allowdrift = TRUE, # include a constant
             trace = TRUE)      # show summary of all models considered
```

Return: Best model: ARIMA(3,1,3)

```{r}
arima313_x2 <- Arima(Data$x2,  # variable	
                  order = c(3, 1, 3),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
AIC(arima510_x2,arima313_x2)
BIC(arima510_x2,arima313_x2)
```

```{r}
arima.best.BIC <- 
  auto.arima(Data$x2,
             d = 1,             # parameter d of ARIMA model
             max.p = 6,         # Maximum value of p
             max.q = 6,         # Maximum value of q
             max.order = 12,    # maximum p+q
             start.p = 1,       # Starting value of p in stepwise procedure
             start.q = 1,       # Starting value of q in stepwise procedure
             ic = "bic",        # Information criterion to be used in model selection.
             stepwise = FALSE,  # if FALSE considers all models
             allowdrift = TRUE, # include a constant
             trace = TRUE)      # show summary of all models considered
```

Return: Best model: ARIMA(3,1,2)

```{r}
arima312_x2 <- Arima(Data$x2,  # variable	
                  order = c(3, 1, 2),  # (p,d,q) parameters
                  include.constant = TRUE)  # including a constant
```

```{r}
AIC(arima510_x2,arima313_x2, arima312_x2)
BIC(arima510_x2,arima313_x2,arima312_x2 )
```

## 4.3 ARIMA Forecast for X1 & X2

Import Data Forecast:

```{r}
out_of_sample <-read.csv("Out_of_sample.csv",header = TRUE, dec = ".")
```

```{r}
class(out_of_sample)
```

Change Date

```{r}
out_of_sample$X <- as.Date(out_of_sample$X, 
                           format = "%d-%m-%y") 
```

Create xts objects

```{r}
out_of_sample <- xts(out_of_sample[,-1], out_of_sample$X)
```

Assign forecast X1 to object oos_x1

```{r}
oos_x1 <- out_of_sample$x1
oos_x1

```

### Forecast for X1

```{r}
tail(Data, 12)
```

```{r}
forecasts_x1 <- forecast(arima313, # model for prediction
                      h = 30) # how many periods outside the sample

forecasts_x1
```

Extract the forecast points

```{r}
forecasts_x1$mean
class(forecasts_x1$mean)
```

It is a ts object, not xts! However, the xts objects are more convenient
and modern. In terms of plotting the real data and forecast data to
compare.

```{r}
forecasts_x1$lower
forecasts_x1$upper
```

Create xts object: (We use the second column (95% confidence interval))

```{r}
forecasts_x1_data <- data.frame(f_mean = as.numeric(forecasts_x1$mean),
                             f_lower = as.numeric(forecasts_x1$lower[, 2]),
                             f_upper = as.numeric(forecasts_x1$upper[, 2]))
```

```{r}
head(forecasts_x1_data,30)
```

Adding real value X1 below the current Dataset:

```{r}
Data_x1 <- rbind(Data[, "x1"], oos_x1)
tail(Data_x1, n = 40)
```

Turn Forecast with Index into Forecast with Date

```{r}
forecasts_x1_xts <- xts(forecasts_x1_data,
                     order.by = index(oos_x1))
forecasts_x1_xts
```

Merge it together with the original data

```{r}
Data_x1_combined <- merge(Data_x1, forecasts_x1_xts)
head(Data_x1_combined)
tail(Data_x1_combined,40)
```

Plot Chart

```{r}
plot(Data_x1_combined [, c("x1", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

```{r}
plot(Data_x1_combined ["2020-11/", c("x1", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

Extract Data for Evaluating the Forecast:

```{r}
Data_x1_Eva <- tail(Data_x1_combined, 30)  
Data_x1_Eva

```

```{r}
Data_x1_Eva$mae   <-  abs(Data_x1_Eva$x1 - Data_x1_Eva$f_mean)
Data_x1_Eva$mse   <-  (Data_x1_Eva$x1 - Data_x1_Eva$f_mean) ^ 2
Data_x1_Eva$mape  <-  abs((Data_x1_Eva$x1 - Data_x1_Eva$f_mean)/Data_x1_Eva$x1)
Data_x1_Eva$amape <-  abs((Data_x1_Eva$x1 - Data_x1_Eva$f_mean)/(Data_x1_Eva$x1 + Data_x1_Eva$f_mean))
Data_x1_Eva
```

```{r}
ARIMA_x1 <- colMeans(Data_x1_Eva[, c("mae", "mse", "mape", "amape")])
apply(Data_x1_Eva[, c("mae", "mse", "mape", "amape")], 2, FUN = median)
```

### Forecast for X2

Assign forecast X2 to object oos_x2

```{r}
oos_x2 <- out_of_sample$x2

oos_x2
```

```{r}
tail(Data, 12)

forecasts_x2 <- forecast(arima510_x2, # model for prediction
                         h = 30) # how many periods outside the sample

forecasts_x2
```

Extract the forecast points

```{r}
forecasts_x2$mean
class(forecasts_x2$mean)
```

It is a ts object, not xts! However, the xts objects are more convenient
and modern. In terms of plotting the real data and forecast data to
compare.

```{r}
forecasts_x2$lower
forecasts_x2$upper
```

Create xts object: (We use the second column (95% confidence interval))

```{r}
forecasts_x2_data <- data.frame(f_mean = as.numeric(forecasts_x2$mean),
                                f_lower = as.numeric(forecasts_x2$lower[, 2]),
                                f_upper = as.numeric(forecasts_x2$upper[, 2]))
```

```{r}
head(forecasts_x2_data,30)
```

Adding real value X1 below the current Dataset:

```{r}
Data_x2 <- rbind(Data[, "x2"], oos_x2)
tail(Data_x2, n = 40)
```

Turn Forecast with Index into Forecast with Date

```{r}
forecasts_x2_xts <- xts(forecasts_x2_data,
                        order.by = index(oos_x2))
forecasts_x2_xts
```

Merge it together with the original data

```{r}
Data_x2_combined <- merge(Data_x2, forecasts_x2_xts)
head(Data_x2_combined)
tail(Data_x2_combined,40)
```

Plot Chart

```{r}
plot(Data_x2_combined ["2020-11/", c("x2", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x2",
     col = c("black", "blue", "red", "red"))
```

Extract Data for Evaluating the Forecast:

```{r}
Data_x2_Eva <- tail(Data_x2_combined, 30)  
Data_x2_Eva
```

```{r}
Data_x2_Eva$mae   <-  abs(Data_x2_Eva$x2 - Data_x2_Eva$f_mean)
Data_x2_Eva$mse   <-  (Data_x2_Eva$x2 - Data_x2_Eva$f_mean) ^ 2
Data_x2_Eva$mape  <-  abs((Data_x2_Eva$x2 - Data_x2_Eva$f_mean)/Data_x2_Eva$x2)
Data_x2_Eva$amape <-  abs((Data_x2_Eva$x2 - Data_x2_Eva$f_mean)/(Data_x2_Eva$x2 + Data_x2_Eva$f_mean))
Data_x2_Eva
```

```{r}
ARIMA_x2 <- colMeans(Data_x2_Eva[, c("mae", "mse", "mape", "amape")])
apply(Data_x2_Eva[, c("mae", "mse", "mape", "amape")], 2, FUN = median)
```

# 5. VECM Model and Analysis

## 5.1. Johansen Cointegration Test

We will assume the K=5 lag structure:

```{r}
johan.test.trace <- 
ca.jo(Data[,1:2], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "trace",  # type of the test: trace or eigen
        K = 5           # lag order of the series (levels) in the VAR
)

summary(johan.test.trace) 

cbind(summary(johan.test.trace)@teststat, 

summary(johan.test.trace)@cval)
```

Test Statistic \< Critical Value: CANNOT reject the null Test Statistic
\> Critical Value: reject the null First we start with r=0, (no
cointegrating vector): t-test statistic \> Critical value: we reject the
null hypothesis about NO cointerating vector. Next, we test the
hypothesis of r=1: Test Statistic \< Critical Value: we CANNOT reject
the null about 1 cointegrating vector. The model has ONLY 1
cointegrating vector.

```{r}
summary(johan.test.trace)@V
```

Weights W:

```{r}
summary(johan.test.trace)@W
```

Check for another type of test: for Eigen:

```{r}
johan.test.eigen <- 
  ca.jo(Data[,1:2], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "eigen",  # type of the test: trace or eigen
        K = 5           # lag order of the series (levels) in the VAR
) 
summary(johan.test.eigen) 
```

The conclusion stays the same: There is only one cointegrating vector.

## 5.2. VECM Model

Define the specification of the VECM model, with cointegrating vector
from either trace or eigen test from Johansen test.

```{r}
Data.vec5 <- cajorls(johan.test.eigen, # defined specification
                        r = 1) # number of cointegrating vectors

summary(Data.vec5)
```

Summary results:

```{r}
summary(Data.vec5$rlm)
```

extract the cointegrating vector:

```{r}
Data.vec5$beta
```

Conclusion about whether Error Correction Mechanism work:

Reparametrizing the VEC model into VAR:

```{r}
Data.vec5.asVAR <- vec2var(johan.test.eigen, r = 1)
```

Check result:

```{r}
Data.vec5.asVAR
```

Calculate and plot Impulse Response Functions:

```{r}
plot(irf(Data.vec5.asVAR, n.ahead = 36), ask = FALSE)
```

Perform variance decomposition:

```{r}
plot(fevd(Data.vec5.asVAR, n.ahead = 36), ask = FALSE)
```

Check if model residuals are autocorrelated or not: Residuals can be
extracted only from the VAR reparametrized model.

```{r}
head(residuals(Data.vec5.asVAR))
serial.test(Data.vec5.asVAR)
```

p-value = 0.0831 \> p-critical = 5% We fail to reject about the null
hypothesis about no-autocorrelation.

-\> There is no auto-correlation.

Plot ACF and PACF for the model:

```{r}
plot(serial.test(Data.vec5.asVAR))
```

Checking the Nomarlity for x1 and x2 by creating Histogram:

```{r}
Data.vec5.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of x1`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(Data.vec5.asVAR)[, 1]), 
                            sd = sd(residuals(Data.vec5.asVAR)[, 1]))) +
  theme_bw() + 
  labs(
    title = "Density of x1 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

```{r}
Data.vec5.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of x2`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(Data.vec5.asVAR)[, 2]), 
                            sd = sd(residuals(Data.vec5.asVAR)[, 2]))) +
  theme_bw() + 
  labs(
    title = "Density of x2 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

We can also check it formally by using the Jarque-Bera (JB) test.

```{r}
normality.test(Data.vec5.asVAR)
```

p-value \> 0.05 =\> Fail to reject Ho about the normality Conclusion:
the residual has a normal distribution.

## 5.3 VECM Forecasting

```{r}
Data.vec5.fore <- 
  predict(
    vec2var(
      johan.test.eigen, 
      r = 1),     # no of cointegrating vectors 
    n.ahead = 30, # forecast horizon
    ci = 0.95)    # confidence level for intervals

summary(Data.vec5.fore)
```

VEC forecasts for x1

```{r}
Data.vec5.fore$fcst$x1
```

VEC forecasts for x2

```{r}
Data.vec5.fore$fcst$x2
```

Lets store it as an xts object. The correct set of dates (index) can be
extracted from the out_of_sample xts data object.

```{r}
tail(index(out_of_sample), 30)
```

```{r}
x1_forecast <- xts(Data.vec5.fore$fcst$x1[,-4], 
                    # we exclude the last column with CI
                    tail(index(out_of_sample), 30))
```

Correction of the variable names:

```{r}
names(x1_forecast) <- c("x1_fore", "x1_lower", "x1_upper")
```

Apply similarly for x2:

```{r}
x2_forecast <- xts(Data.vec5.fore$fcst$x2[, -4],
                    # we exclude the last column with CI
                    tail(index(out_of_sample), 30))

names(x2_forecast) <- c("x2_fore", "x2_lower", "x2_upper")

```

Merge forecast into orignial data:

```{r}
Data.fore <- merge(out_of_sample[,1:2], 
                 x1_forecast,
                 x2_forecast)
```

```{r}
tail(Data.fore,40) 
```

Plot chart for Forecast:

```{r}
plot(Data.fore ["2020-11/", c("x1", "x1_fore", "x1_lower", "x1_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

```{r}
plot(Data.fore ["2020-11/", c("x2", "x2_fore", "x2_lower", "x2_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x2",
     col = c("black", "blue", "red", "red"))
```

## 5.4. Evaluate Forecast Accuracy

Extract the out-of-sample data to evaluate:

```{r}
Data.fore2 <- Data.fore[,-30]
```

```{r}
Data.fore2$mae.x1   <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)
Data.fore2$mse.x1   <-  (Data.fore2$x1 - Data.fore2$x1_fore)^2
Data.fore2$mape.x1  <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)/Data.fore2$x1
Data.fore2$amape.x1 <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)/(Data.fore2$x1 + Data.fore2$x1_fore)
```

```{r}
Data.fore2$mae.x2   <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)
Data.fore2$mse.x2   <-  (Data.fore2$x2 - Data.fore2$x2_fore)^2
Data.fore2$mape.x2  <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)/Data.fore2$x2
Data.fore2$amape.x2 <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)/(Data.fore2$x2 + Data.fore2$x2_fore)
```

and calculate its averages

```{r}
VECM_x1 <- colMeans(Data.fore2[, c("mae.x1", 
                      "mse.x1",
                      "mape.x1",
                      "amape.x1")], na.rm = TRUE)
```

```{r}
VECM_x2 <- colMeans(Data.fore2[, c("mae.x2", 
                      "mse.x2",
                      "mape.x2",
                      "amape.x2")], na.rm = TRUE)  
```

# 6. Comparing Models

Comparing VECM model's forecasts with ARIMAs:

```{r}
result <- rbind(ARIMA_x1,VECM_x1, ARIMA_x2,VECM_x2)

result %>%
  knitr::kable(digits = 4) %>%
  kableExtra::kable_styling(full_width = F,
                            bootstrap_options = c("striped", 
                                                  "hover", 
                                                  "condensed"))

```

Conclusion: The forecasts from ARIMA model outperforms that of VECM.
