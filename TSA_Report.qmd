---
title: "FORECASTING FINANCIAL INSTRUMENTS PRICES WITH VECM AND ARIMA MODELS"
subtitle: "Home Taken Project 1"
author: "Nhan Nguyen - Shivam Varshney"
date: today
format: 
  html:
    toc: true
    toc-depth: 2
    smooth-scroll: true
    theme:
      light: flatly
      dark: darkly
  pdf:
    lof: true
    lot: true
    pdf-engine: pdflatex
    latex-tinytex: false
editor: visual
---

## TSA_Report

```{r}
#| echo: false
getwd()
```

```{r}
#| echo: false
setwd("D://UW//2. Summer 22-23//1. Time Series Analysis//Project//TSA")
```

# 1. Data Preparation

Load necessary packages to memory and some pre-defined functions::

```{r}
#| echo: false
library(xts)
library(tidyverse)
library(quantmod)
library(lmtest)
library(forecast)
library(dygraphs)
library(vars)
library(knitr)
library(kableExtra)
source("testdf.R")
source("function_plot_ACF_PACF_resids.R")
```

Load Dataset:

```{r}
Data <- read.csv("TSA_2023_project_data_1.csv",header = TRUE, dec = ".")
```

# 2. Summary Data

```{r}
str(Data)
```

Notice that the first column is the Date column, which is under "Character" type We need to transform it into "Date" type.

```{r}
Data$X <- as.Date(Data$X, 
                  format = "%d-%m-%y")
```

Until now the class is "Data.Frame" object

```{r}
class(Data) 
```

Create xts objects

```{r}
Data <- xts(Data[,-1], Data$X)
```

After creating xts objects, the class is "xts", "zoo"

```{r}
class(Data) 
```

```{r}
str(Data)
```

Checking for missing data

```{r}
summary(Data)
```

```{r}
head(Data,6)
```

There is no missing data for the table.

# 3. Checking for Cointegration

## Visualization

Plot 10 Time Series together

```{r}
plot(Data,  
     main = "Graph for 10 Time Series",  
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     legend.loc = "bottomleft",
     type="l")
```

Plot 10 Time Series separately

```{r}
plot(Data,
     main = "Graph for 10 Time Series",  
     major.ticks = "years", 
     grid.ticks.on = "years",
     legend.loc = "bottomleft",
     multi.panel = TRUE,
     yaxis.same = FALSE,
     type="l")
```

So our group decide to choose Time Series 1 and 2 for analysis.

## Create first difference

```{r}
Data$dx1 <- diff.xts(Data$x1)
Data$dx2 <- diff.xts(Data$x2)
```

Plot both variables on the graph:

```{r}
plot(Data[, 1:2],
     col = c("black", "blue"),
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 2,
     main = "Time Series X1 & X2",
     legend.loc = "topright")
```

## Test cointegration

We perform the tests of integration order.

Testing the order of the time series 1 and its difference:

```{r}
testdf(variable = Data$x1,
       max.augmentations = 3)
```

Interpret result: The p-bg is very small -\> there are auto-correlation in residuals, even when we add the augmentations, we still cant get rid of auto-correlation. Hence, we test for its 1st difference.

```{r}
testdf(variable = Data$dx1, 
       max.augmentations = 3)
```

The p-bg is greater than 5% -\> there are no auto-correlations in residual Next, we test the stationary of its 1st lag, by checking p-adf. p-adf is smaller than 5%. We can reject the null in the case of the first differences about non-stationary. Its 1st lag is stationary. We can conclude that the Time Series X1 is integrated of order 1.

Testing the order of the time series 2 and its difference:

```{r}
testdf(variable = Data$x2,
       max.augmentations = 3)
```

Interpret result: Similar to X1, the p-bg here is smaller than 5% -\> there are auto-correlation in residuals, even when we add the augmentations, we still cant get rid of auto-correlation. Hence, we test for its 1st difference.

```{r}
testdf(variable = Data$dx2, 
       max.augmentations = 3)
```

By adding 1 augmentation, there will be no auto correlation in Residuals. p-adf is greater than 5%, we can reject the null hypothesis in the case the non-stationary of first differences. Its 1st lag is stationary. we can conclude that the Time Series X2 is integrated of order 1.

**Conclusion:** As a result, both variables has the same order 1: âˆ¼I(1) so in the next step we can check whether they are cointegrated or not.

## Estimate Cointegrated Vector

Granger Causality Test
( Ho:	X does not granger cause Y
  H1:	X granger causes Y)
```{r}
grangertest(x1 ~ x2,
            data = Data,
            order = 3) # lag assumed
```
p-value is big, we cant reject Ho that X2 does not granger cause X1.

```{r}
grangertest(x2 ~ x1,
            data = Data,
            order = 3) # lag assumed
```
p-value is big, we reject Ho, and conclude that X1 does granger cause X2.

**Conclusion:** At 5% significance level (or 95% confidence level) we have so called one-directional feedback, X1 does granger cause X2.

```{r}
model.coint <- lm(x2 ~ x1, data = Data)
```

Examine the model summary:

```{r}
summary(model.coint)
```

Both the intercept and Coefficient for x1 are statistically significant.

The model is significantly explained by x1. We further test the stationary of the residuals:

```{r}
testdf(variable = residuals(model.coint), max.augmentations = 3)
```

The ADF test with no augmentations can be used. The result is that non-stationarity of residuals is STRONGLY REJECTED, so residuals are stationary, which means that x1 and x2 are cointegrated.

The cointegrating vector is \[1, 16.127 , -1.145\]

which defines the cointegrating relationship as: 1 \* x2 + 16.127 - 1.145 \* x1.

Create first lags of residuals and adding them to the dataset
```{r}
Data$lresid <- lag.xts(residuals(model.coint))
```

***Estimating ECM Model***
As the intercept is insignificant, we can remove it. 

```{r}
model.ecm <- lm(dx2 ~ dx1 + lresid -1, data = Data)
summary(model.ecm)
```

The parameter **0.021** describes a **short term** relationship between x1 and x2.

The parameter **1.145** describes **a long term** relationship between x1 and x2.

The value of **-0.144** is the estimate of the adjustment coefficient. 
Its sign is **negative** and this value means that 0.139 of the unexpected error (increase in gap) will be corrected in the next period, so any unexpected deviation should be corrected finally on average within about **6.9 periods**.

# 4. Applying Box-Jenkins Procedure

## 4.1 Applying for Time Series X1

### Step 1: Model Parameters

```{r}
par(mfrow = c(2, 1)) 
acf(Data$dx1,
    lag.max = 36, # max lag for ACF
    ylim = c(-0.1, 0.1),   # limits for the y axis - we give c(min, max)
    lwd = 5,               
    col = "dark green",
    na.action = na.pass)   # do not stop if there are missing values in the data
pacf(Data$dx1, 
     lag.max = 36, 
     lwd = 5, col = "dark green",
     na.action = na.pass)

par(mfrow = c(1, 1)) # restore the original single panel
```

The PACF shown is suggestive of an AR(5) model or AR(7). So an initial candidate model is an ARIMA(5,1,0). We also have some variations of this model: ARIMA(5,1,1), ARIMA(4,1,0), ARIMA(3,1,0), ARIMA(7,1,0).

### Step 2: Model Estimation

```{r}
arima510 <- Arima(Data$x1,  
                  order = c(5, 1, 0) 	
)
```

```{r}
arima511 <- Arima(Data$x1,	
                  order = c(5, 1, 1)
)
```

```{r}
arima410 <- Arima(Data$x1, 
                  order = c(4, 1, 0)
)
```

```{r}
arima310 <- Arima(Data$x1,  
                  order = c(3, 1, 0)
)
```

```{r}
arima313 <- Arima(Data$x1,
                  order = c(3, 1, 3)
)
```

```{r}
arima710 <- Arima(Data$x1,	
                  order = c(7, 1, 0),
)	
```

The above model is not included constant. Let's include constant into the model:

```{r}
arima510_2 <- Arima(Data$x1,	
                  order = c(5, 1, 0),
                  include.constant = TRUE)  # including a constant
```

```{r}
coeftest(arima510_2)
```

Adding the constant did not change the result much for ALL MODELS, so we keep the model without constant.

\*\*\*Summary All The Models:\*\*\*

```{r}
coeftest(arima510)
```

All the terms/coeffs are significant.

```{r}
coeftest(arima511)
```

All the terms/coeffs are significant, except MA1.

```{r}
coeftest(arima410)
```

All the terms/coeffs are significant.

```{r}
coeftest(arima310)
```

All the terms/coeffs are significant.

```{r}
coeftest(arima313)
```

All the terms/coeffs are significant, except MA1, MA3.

```{r}
coeftest(arima710)
```

All the terms/coeffs are significant, except AR5, AR7.

### Step 3: Model Diagnostics

```{r}
plot(resid(arima510),col = "royalblue")
```

```{r}
plot(resid(arima511),col = "royalblue")
```

```{r}
plot(resid(arima410),col = "royalblue")
```

```{r}
plot(resid(arima310),col = "royalblue")
```

```{r}
plot(resid(arima313), col = "royalblue3")
```

```{r}
plot(resid(arima710),col = "royalblue")
```

Lets check the ACF and the PACF of the Residual values:

```{r}
plot_ACF_PACF_resids(arima510)
```

```{r}
plot_ACF_PACF_resids(arima511)
```

```{r}
plot_ACF_PACF_resids(arima410)
```

```{r}
plot_ACF_PACF_resids(arima310)
```

```{r}
plot_ACF_PACF_resids(arima313)
```

```{r}
plot_ACF_PACF_resids(arima710)
```

The Ljung-Box test:

```{r}
Box.test(resid(arima510), type = "Ljung-Box", lag = 10)
Box.test(resid(arima510), type = "Ljung-Box", lag = 15)
Box.test(resid(arima510), type = "Ljung-Box", lag = 20)
Box.test(resid(arima510), type = "Ljung-Box", lag = 25)
```

```{r}
Box.test(resid(arima511), type = "Ljung-Box", lag = 10)
Box.test(resid(arima511), type = "Ljung-Box", lag = 15)
Box.test(resid(arima511), type = "Ljung-Box", lag = 20)
Box.test(resid(arima511), type = "Ljung-Box", lag = 25)
```

```{r}
Box.test(resid(arima410), type = "Ljung-Box", lag = 10)
Box.test(resid(arima410), type = "Ljung-Box", lag = 15)
Box.test(resid(arima410), type = "Ljung-Box", lag = 20)
Box.test(resid(arima410), type = "Ljung-Box", lag = 25)
```

```{r}
Box.test(resid(arima310), type = "Ljung-Box", lag = 10)
Box.test(resid(arima310), type = "Ljung-Box", lag = 15)
Box.test(resid(arima310), type = "Ljung-Box", lag = 20)
Box.test(resid(arima310), type = "Ljung-Box", lag = 25)
```

We have very low p-values, greater than 5% -\> Reject Ho about no autocorrelation

Hence, the residual is auto-correlated in model ARIMA(3,1,0).

```{r}
Box.test(resid(arima313), type = "Ljung-Box", lag = 10)
Box.test(resid(arima313), type = "Ljung-Box", lag = 15)
Box.test(resid(arima313), type = "Ljung-Box", lag = 20)
Box.test(resid(arima313), type = "Ljung-Box", lag = 25)
```

```{r}
Box.test(resid(arima710), type = "Ljung-Box", lag = 10)
Box.test(resid(arima710), type = "Ljung-Box", lag = 15)
Box.test(resid(arima710), type = "Ljung-Box", lag = 20)
Box.test(resid(arima710), type = "Ljung-Box", lag = 25)
```

We have very large p-values, greater than 5% for all models (except ARIMA(3,1,0)) -\> fail to reject Ho about no autocorrelation

Hence, the residual is white-noise.

Plot graph for Ljung-Box test:

For model ARIMA(5,1,0)

```{r}
bj_pvalues = c()
for(i in c(1:100)){
  bj = Box.test(resid(arima510), type = "Ljung-Box", lag = i)
  bj_pvalues = append(bj_pvalues,bj$p.value)
}

plot(bj_pvalues, type='l')

abline(h = 0.05, col='red')

```

ARIMA(3,1,3)

```{r}
bj_pvalues = c()

for(i in c(1:100)){
  bj = Box.test(resid(arima313), type = "Ljung-Box", lag = i)
  bj_pvalues = append(bj_pvalues,bj$p.value)
}

plot(bj_pvalues, type='l')

abline(h = 0.05, col='red')
```

### Step 4. Evaluate Model

```{r}
AIC(arima510, arima511,
    arima410, 
    arima310, arima313,
    arima710)
```

Model ARIMA(3,1,3) returns the lowest AIC test.

```{r}
BIC(arima510, arima511,
    arima410,
    arima310, arima313,
    arima710)
```

Model ARIMA(5,1,0) returns the lowest BIC test; Model ARIMA(3,1,3) comes second.

However, we should prefer AIC over BIC.

***Conclusion:*** From the perspective of sensibility, the ARIMA(3,1,3) seems to be the most attractive one: all terms are significant, residuals are white noise and we observe low values of information criteria (IC).

Hence, we use ARIMA(3,1,3) for forecasting Time Series X1.

## 4.1 Applying for Time Series X2

### Step 1: Model Parameters

```{r}
par(mfrow = c(2, 1)) 
acf(Data$dx2,
    lag.max = 36, # max lag for ACF
    ylim = c(-0.1, 0.1),   # limits for the y axis - we give c(min, max)
    lwd = 5,               
    col = "dark green",
    na.action = na.pass)   # do not stop if there are missing values in the data
pacf(Data$dx2, 
     lag.max = 36, 
     lwd = 5, col = "dark green",
     na.action = na.pass)

par(mfrow = c(1, 1)) # restore the original single panel

```

The PACF shown is suggestive of an AR(5) model. So an initial candidate model is an ARIMA(5,1,0). We also have some variations of this model: ARIMA(5,1,1), ARIMA(4,1,0), ARIMA(3,1,0).

### Step 2: Model Estimation

```{r}
arima510_x2 <- Arima(Data$x2,	
                  order = c(5, 1, 0)
)	
```

```{r}
arima511_x2 <- Arima(Data$x2,
                     order = c(5, 1, 1)
)
```

```{r}
arima410_x2 <- Arima(Data$x2,
                     order = c(4, 1, 0)
)
```

```{r}
arima310_x2 <- Arima(Data$x2,
                     order = c(3, 1, 0)
)
```

```{r}
arima313_x2 <- Arima(Data$x2,
                     order = c(3, 1, 3)
)
```

The above model is not included constant. Let's include constant into the model:

```{r}
arima510_2_x2 <- Arima(Data$x2,
                    order = c(5, 1, 0),
                    include.constant = TRUE)
```

Adding the constant did not change the result much for ALL MODELS, so we keep the model without constant.

***Summary Results:***

```{r}
coeftest(arima510_x2)
```

All parameters are significant.

```{r}
coeftest(arima511_x2)
```

All parameters are significant, except MA1.

```{r}
coeftest(arima410_x2)
```

All parameters are significant.

```{r}
coeftest(arima310_x2)
```

All parameters are significant.

```{r}
coeftest(arima313_x2)
```

All parameters are significant, except MA3.

### Step 3: Model Diagnostics

```{r}
plot(resid(arima510_x2))
```

```{r}
plot(resid(arima511_x2))
```

```{r}
plot(resid(arima410_x2))
```

```{r}
plot(resid(arima310_x2))
```

```{r}
plot(resid(arima313_x2))
```

***The Ljung-Box test:***

```{r}
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima510_x2), type = "Ljung-Box", lag = 25)
```

-\> Very large p-values: The residual is white-noise

```{r}
Box.test(resid(arima511_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima511_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima511_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima511_x2), type = "Ljung-Box", lag = 25)
```

-\> Very large p-values: The residual is white-noise

```{r}
Box.test(resid(arima410_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima410_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima410_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima410_x2), type = "Ljung-Box", lag = 25)
```

-\> Not so large p-values: The residual is not really a white-noise for lag20.

```{r}
Box.test(resid(arima310_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima310_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima310_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima310_x2), type = "Ljung-Box", lag = 25)
```

-\> Very small p-values: The residual is auto-correlated

```{r}
Box.test(resid(arima313_x2), type = "Ljung-Box", lag = 10)
Box.test(resid(arima313_x2), type = "Ljung-Box", lag = 15)
Box.test(resid(arima313_x2), type = "Ljung-Box", lag = 20)
Box.test(resid(arima313_x2), type = "Ljung-Box", lag = 25)
```

-\> Very large p-values: The residual is white-noise

Plot graph for Ljung-Box test:

```{r}
bj_pvalues = c()

for(i in c(1:100)){
  bj = Box.test(resid(arima510_x2), type = "Ljung-Box", lag = i)
  bj_pvalues = append(bj_pvalues,bj$p.value)
}

plot(bj_pvalues, type='l')

abline(h=0.05, col='red')
```

### Step 4. Evaluate Model

```{r}
AIC(arima510_x2,arima511_x2,
    arima410_x2,  
    arima310_x2, arima313_x2)

```

-\> Suggestion model: arima313_x2 - ARIMA(3,1,3)

```{r}
BIC(arima510_x2,arima511_x2,
    arima410_x2,  
    arima310_x2, arima313_x2)
```

-\> Suggestion model: arima510_x2 - ARIMA(5,1,0)

***Conclusion:*** From the perspective of sensibility, the ARIMA(5,1,0) seems to be the most attractive one:

all terms are significant, residuals are white noise and we observe low values of information criteria (IC).

-\> Suggestion model: arima510_x2 - ARIMA(5,1,0)

## 4.3 ARIMA Forecast for X1 & X2

Import Data Forecast:

```{r}
out_of_sample <-read.csv("Out_of_sample.csv",header = TRUE, dec = ".")
```

```{r}
class(out_of_sample)
```

Change Date

```{r}
out_of_sample$X <- as.Date(out_of_sample$X, 
                           format = "%d-%m-%y") 
```

Create xts objects

```{r}
out_of_sample <- xts(out_of_sample[,-1], out_of_sample$X)
```

Assign forecast X1 to object oos_x1

```{r}
oos_x1 <- out_of_sample$x1
oos_x1
```

### Forecast for X1

```{r}
tail(Data, 12)
```

```{r}
forecasts_x1 <- forecast(arima313, # model for prediction
                      h = 30) # how many periods outside the sample

forecasts_x1
```

Extract the forecast points

```{r}
forecasts_x1$mean
class(forecasts_x1$mean)
```

It is a ts object, not xts! However, the xts objects are more convenient and modern. In terms of plotting the real data and forecast data to compare.

```{r}
forecasts_x1$lower
forecasts_x1$upper
```

Create xts object: (We use the second column (95% confidence interval))

```{r}
forecasts_x1_data <- data.frame(f_mean = as.numeric(forecasts_x1$mean),
                             f_lower = as.numeric(forecasts_x1$lower[, 2]),
                             f_upper = as.numeric(forecasts_x1$upper[, 2]))
```

```{r}
head(forecasts_x1_data,30)
```

Adding real value X1 below the current Dataset:

```{r}
Data_x1 <- rbind(Data[, "x1"], oos_x1)
tail(Data_x1, n = 40)
```

Turn Forecast with Index into Forecast with Date

```{r}
forecasts_x1_xts <- xts(forecasts_x1_data,
                     order.by = index(oos_x1))
forecasts_x1_xts
```

Merge it together with the original data

```{r}
Data_x1_combined <- merge(Data_x1, forecasts_x1_xts)
head(Data_x1_combined)
tail(Data_x1_combined,40)
```

Plot Chart

```{r}
plot(Data_x1_combined [, c("x1", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

```{r}
plot(Data_x1_combined ["2020-11/", c("x1", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

Extract Data for Evaluating the Forecast:

```{r}
Data_x1_Eva <- tail(Data_x1_combined, 30)  
Data_x1_Eva

```

```{r}
Data_x1_Eva$mae   <-  abs(Data_x1_Eva$x1 - Data_x1_Eva$f_mean)
Data_x1_Eva$mse   <-  (Data_x1_Eva$x1 - Data_x1_Eva$f_mean) ^ 2
Data_x1_Eva$mape  <-  abs((Data_x1_Eva$x1 - Data_x1_Eva$f_mean)/Data_x1_Eva$x1)
Data_x1_Eva$amape <-  abs((Data_x1_Eva$x1 - Data_x1_Eva$f_mean)/(Data_x1_Eva$x1 + Data_x1_Eva$f_mean))
Data_x1_Eva
```

```{r}
ARIMA_x1 <- colMeans(Data_x1_Eva[, c("mae", "mse", "mape", "amape")])
apply(Data_x1_Eva[, c("mae", "mse", "mape", "amape")], 2, FUN = median)
```

### Forecast for X2

Assign forecast X2 to object oos_x2

```{r}
oos_x2 <- out_of_sample$x2
oos_x2
```

```{r}
tail(Data, 12)

forecasts_x2 <- forecast(arima510_x2, # model for prediction
                         h = 30) # how many periods outside the sample
forecasts_x2
```

Extract the forecast points

```{r}
forecasts_x2$mean
class(forecasts_x2$mean)
```

It is a ts object, not xts! However, the xts objects are more convenient and modern. In terms of plotting the real data and forecast data to compare.

```{r}
forecasts_x2$lower
forecasts_x2$upper
```

Create xts object: (We use the second column (95% confidence interval))

```{r}
forecasts_x2_data <- data.frame(f_mean = as.numeric(forecasts_x2$mean),
                                f_lower = as.numeric(forecasts_x2$lower[, 2]),
                                f_upper = as.numeric(forecasts_x2$upper[, 2]))
```

```{r}
head(forecasts_x2_data,30)
```

Adding real value X2 below the current Dataset:

```{r}
Data_x2 <- rbind(Data[, "x2"], oos_x2)
tail(Data_x2, n = 40)
```

Turn Forecast with Index into Forecast with Date

```{r}
forecasts_x2_xts <- xts(forecasts_x2_data,
                        order.by = index(oos_x2))
forecasts_x2_xts
```

Merge it together with the original data

```{r}
Data_x2_combined <- merge(Data_x2, forecasts_x2_xts)
head(Data_x2_combined)
tail(Data_x2_combined,40)
```

Plot Chart

```{r}
plot(Data_x2_combined ["2020-11/", c("x2", "f_mean", "f_lower", "f_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x2",
     col = c("black", "blue", "red", "red"))
```

Extract Data for Evaluating the Forecast:

```{r}
Data_x2_Eva <- tail(Data_x2_combined, 30)  
Data_x2_Eva
```

```{r}
Data_x2_Eva$mae   <-  abs(Data_x2_Eva$x2 - Data_x2_Eva$f_mean)
Data_x2_Eva$mse   <-  (Data_x2_Eva$x2 - Data_x2_Eva$f_mean) ^ 2
Data_x2_Eva$mape  <-  abs((Data_x2_Eva$x2 - Data_x2_Eva$f_mean)/Data_x2_Eva$x2)
Data_x2_Eva$amape <-  abs((Data_x2_Eva$x2 - Data_x2_Eva$f_mean)/(Data_x2_Eva$x2 + Data_x2_Eva$f_mean))
Data_x2_Eva
```

```{r}
ARIMA_x2 <- colMeans(Data_x2_Eva[, c("mae", "mse", "mape", "amape")])
apply(Data_x2_Eva[, c("mae", "mse", "mape", "amape")], 2, FUN = median)
```

# 5. VECM Model and Analysis

## 5.1. Johansen Cointegration Test

Determine the lag for Johansen test:

```{r}
VARselect(Data[,1:2], 
          lag.max = 7)
```

Three out of four tests suggests K = 6.

We will choose the K=6 lag structure:

```{r}
johan.test.trace <- 
ca.jo(Data[,1:2], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "trace",  # type of the test: trace or eigen
        K = 6           # lag order of the series (levels) in the VAR
)

summary(johan.test.trace) 

cbind(summary(johan.test.trace)@teststat, 

summary(johan.test.trace)@cval)
```

Test Statistic \< Critical Value: CANNOT reject the null

Test Statistic \> Critical Value: reject the null

First we start with r=0, (no cointegrating vector): t-test statistic \> Critical value: we reject the null hypothesis about NO cointerating vector.

Next, we test the hypothesis of r=1: Test Statistic \< Critical Value: we CANNOT reject the null about 1 cointegrating vector.

The model has ONLY 1 cointegrating vector.

```{r}
summary(johan.test.trace)@V
```

Weights W:

```{r}
summary(johan.test.trace)@W
```

Check for another type of test: for Eigen:

```{r}
johan.test.eigen <- 
  ca.jo(Data[,1:2], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "eigen",  # type of the test: trace or eigen
        K = 6           # lag order of the series (levels) in the VAR
) 
summary(johan.test.eigen) 
```

The conclusion stays the same: There is only one cointegrating vector.

## 5.2. VECM Model

Define the specification of the VECM model, with cointegrating vector from either trace or eigen test from Johansen test.

```{r}
Data.vec6 <- cajorls(johan.test.eigen, # defined specification
                        r = 1) # number of cointegrating vectors

summary(Data.vec6)
```

Summary results:

```{r}
summary(Data.vec6$rlm)
```

extract the cointegrating vector:

```{r}
Data.vec6$beta
```

extract the adjustment coefficients (check for sign to determine whether ECM works or not):

```{r}
johan.test.eigen@W
```

Conclusion about whether Error Correction Mechanism work: The adjustment Coeff has different sign here -\> ECM works.

Reparametrizing the VEC model into VAR:

```{r}
Data.vec6.asVAR <- vec2var(johan.test.eigen, r = 1)
```

Check result:

```{r}
Data.vec6.asVAR
```

Calculate and plot Impulse Response Functions:

```{r}
plot(irf(Data.vec6.asVAR, n.ahead = 36), ask = FALSE)
```

The residuals seem to be stable: first increases then decreases.

Perform variance decomposition:

```{r}
plot(fevd(Data.vec6.asVAR, n.ahead = 36), ask = FALSE)
```

Check if model residuals are autocorrelated or not: Residuals can be extracted only from the VAR reparametrized model.

```{r}
head(residuals(Data.vec6.asVAR))
serial.test(Data.vec6.asVAR)
```

p-value = 0.6616 \> p-critical = 5%

The null about no-autocorrelation is fail to Reject.

=\> There is no auto-correlation in Residuals.

Plot ACF and PACF for the model:

```{r}
plot(serial.test(Data.vec6.asVAR))
```

Checking the Nomarlity for x1 and x2 by creating Histogram:

```{r}
Data.vec6.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of x1`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(Data.vec6.asVAR)[, 1]), 
                            sd = sd(residuals(Data.vec6.asVAR)[, 1]))) +
  theme_bw() + 
  labs(
    title = "Density of x1 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

```{r}
Data.vec6.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of x2`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(Data.vec6.asVAR)[, 2]), 
                            sd = sd(residuals(Data.vec6.asVAR)[, 2]))) +
  theme_bw() + 
  labs(
    title = "Density of x2 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

We can also check it formally by using the Jarque-Bera (JB) test.

```{r}
normality.test(Data.vec6.asVAR)
```

p-value \> 0.05 =\> Fail to reject Ho about the normality

Conclusion: the residual has a normal distribution.

## 5.3 VECM Forecasting

```{r}
Data.vec6.fore <- 
  predict(
    vec2var(
      johan.test.eigen, 
      r = 1),     # no of cointegrating vectors 
    n.ahead = 30, # forecast horizon
    ci = 0.95)    # confidence level for intervals

summary(Data.vec6.fore)
```

VEC forecasts for x1

```{r}
Data.vec6.fore$fcst$x1
```

VEC forecasts for x2

```{r}
Data.vec6.fore$fcst$x2
```

Lets store it as an xts object. The correct set of dates (index) can be extracted from the out_of_sample xts data object.

```{r}
tail(index(out_of_sample), 30)
```

```{r}
x1_forecast <- xts(Data.vec6.fore$fcst$x1[,-4], 
                    # we exclude the last column with CI
                    tail(index(out_of_sample), 30))
```

Correction of the variable names:

```{r}
names(x1_forecast) <- c("x1_fore", "x1_lower", "x1_upper")
```

Apply similarly for x2:

```{r}
x2_forecast <- xts(Data.vec6.fore$fcst$x2[, -4],
                    # we exclude the last column with CI
                    tail(index(out_of_sample), 30))

names(x2_forecast) <- c("x2_fore", "x2_lower", "x2_upper")

```

Merge forecast into orignial data:

```{r}
Data.fore <- merge(out_of_sample[,1:2], 
                 x1_forecast,
                 x2_forecast)
```

```{r}
tail(Data.fore,40) 
```

Plot chart for Forecast:

```{r}
plot(Data.fore ["2020-11/", c("x1", "x1_fore", "x1_lower", "x1_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x1",
     col = c("black", "blue", "red", "red"))
```

```{r}
plot(Data.fore ["2020-11/", c("x2", "x2_fore", "x2_lower", "x2_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "30 day forecast of x2",
     col = c("black", "blue", "red", "red"))
```

## 5.4. Evaluate Forecast Accuracy

Extract the out-of-sample data to evaluate:

```{r}
Data.fore2 <- Data.fore[,-30]
```

```{r}
Data.fore2$mae.x1   <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)
Data.fore2$mse.x1   <-  (Data.fore2$x1 - Data.fore2$x1_fore)^2
Data.fore2$mape.x1  <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)/Data.fore2$x1
Data.fore2$amape.x1 <-  abs(Data.fore2$x1 - Data.fore2$x1_fore)/(Data.fore2$x1 + Data.fore2$x1_fore)
```

```{r}
Data.fore2$mae.x2   <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)
Data.fore2$mse.x2   <-  (Data.fore2$x2 - Data.fore2$x2_fore)^2
Data.fore2$mape.x2  <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)/Data.fore2$x2
Data.fore2$amape.x2 <-  abs(Data.fore2$x2 - Data.fore2$x2_fore)/(Data.fore2$x2 + Data.fore2$x2_fore)
```

and calculate its averages

```{r}
VECM_x1 <- colMeans(Data.fore2[, c("mae.x1", 
                      "mse.x1",
                      "mape.x1",
                      "amape.x1")], na.rm = TRUE)
```

```{r}
VECM_x2 <- colMeans(Data.fore2[, c("mae.x2", 
                      "mse.x2",
                      "mape.x2",
                      "amape.x2")], na.rm = TRUE)  
```

# 6. Comparing Models

Comparing VAR model's forecasts with ARIMAs:

```{r}
result <- rbind(ARIMA_x1,VECM_x1, ARIMA_x2,VECM_x2)

result %>%
  knitr::kable(digits = 4) %>%
  kableExtra::kable_styling(full_width = F,
                            bootstrap_options = c("striped", 
                                                  "hover", 
                                                  "condensed"))
```

**Conclusion:**

For Timeseries x1: The forecasts from VECM outperforms that of ARIMA.

For Timeseries x2: The forecasts from ARIMA model outperforms that of VECM.
